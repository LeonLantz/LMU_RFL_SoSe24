{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Environment Name\n",
    "environment_name = 'Pendulum-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåé **Zuf√§llige Action-Auswahl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate(env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        while not done and not terminated:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "env = gym.make(environment_name)\n",
    "returns = evaluate(env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            done = done or terminated\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø **Data Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for column names\n",
    "COS_ANGLE = \"cosAngle\"\n",
    "SIN_ANGLE = \"sinAngle\"\n",
    "ANG_VEL = \"angVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "def sample_data(episodes=10000, seed=0):\n",
    "    # Create the Pendulum environment\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    \n",
    "    # Create an empty list to store transitions\n",
    "    transitions = []\n",
    "\n",
    "    # Sample data\n",
    "    for episode in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            transitions.append({COS_ANGLE: obs[0], SIN_ANGLE: obs[1], \n",
    "                                ANG_VEL: obs[2], EPISODE: episode, \n",
    "                                STEP: step, ACTION: action[0]})\n",
    "\n",
    "            obs, reward, done, terminated, _ = env.step(action)\n",
    "            done = done or terminated\n",
    "\n",
    "    # Convert the list of transitions to a Pandas DataFrame\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "# Sample data\n",
    "df = sample_data(episodes=200, seed=0)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10,15), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è **Cut Out Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Data density**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è **Model Based RFL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#### HELPER FUNCTIONS FOR PATTERN GENERATION\n",
    "#############################################\n",
    "def create_training_data(data, input_col, target_col, window_size=1, training_pattern_percent=0.7):\n",
    "\n",
    "    data_train = data\n",
    "\n",
    "    mean_in, std_in = mean_and_std(input_col, data_train)\n",
    "    mean_out, std_out = mean_and_std(target_col, data_train)\n",
    "    #data_plot.plot_hist_df(data_train, input_col)\n",
    "    #data_plot.plot_timeseries_df(data_train, input_col)\n",
    "    print(f\"mean in = {mean_in}\" )\n",
    "    print(f\"std in = {std_in}\")\n",
    "    print(f\"mean out =  {mean_out}\")\n",
    "    print(f\"std out = {std_out}\")\n",
    "\n",
    "    grouped = data_train.groupby(['episode'])\n",
    "\n",
    "    inputs_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    for g in grouped:\n",
    "        # be sure that data inside a group is not shuffled # not sure if needed\n",
    "        g = g[1].sort_values(by='step')\n",
    "\n",
    "        past_history = window_size   # t-3, t-2, t-1, t\n",
    "        future_target = 0  # t+1\n",
    "        STEP = 1 # no subsampling of rows in data, e.g. only every i'th row\n",
    "\n",
    "        # use pandas.DataFrame.values in order to get an numpy array from an pandas.DataFrame object\n",
    "\n",
    "        inputs, labels = multivariate_data(dataset=g[input_col][:].values, target=g[target_col][:].values,\n",
    "                                        start_index=0, end_index=g[input_col][:].values.shape[0]-future_target,\n",
    "                                        history_size=past_history, target_size=future_target, step=STEP,\n",
    "                                        single_step=True)\n",
    "\n",
    "        ## Append data to whole set of patterns\n",
    "        for i in range (0, len(inputs)):\n",
    "            inputs_all.append(inputs[i])\n",
    "            labels_all.append(labels[i])\n",
    "  \n",
    "    length = len(inputs_all)\n",
    "\n",
    "    c = list(zip(inputs_all, labels_all))\n",
    "    np.random.shuffle(c)\n",
    "    inputs_all, labels_all = zip(*c)\n",
    "\n",
    "    split = int(training_pattern_percent * length)\n",
    "\n",
    "    inputs_all = np.array(inputs_all)\n",
    "    labels_all = np.array(labels_all)\n",
    "\n",
    "    return ((inputs_all[0:split], labels_all[0:split]), (inputs_all[split:], labels_all[split:])), mean_in, std_in, mean_out, std_out\n",
    "\n",
    "\n",
    "def mean_and_std(columns, data):\n",
    "    mean = np.zeros(len(columns))\n",
    "    std = np.zeros(len(columns))\n",
    "    index = 0\n",
    "    for c in columns:\n",
    "        mean[index], std[index] = get_normalizations(data[c])\n",
    "        index = index + 1\n",
    "    return mean, std\n",
    "\n",
    "def get_normalizations(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return mean, std\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "       end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data, dtype=np.float32), np.array(labels, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(df, input_col, target_col, window_size, training_batch_size=50, validation_batch_size=50, training_pattern_percent=0.7):\n",
    "    \n",
    "    global x_train_multi, y_train_multi\n",
    "    \n",
    "    ###################\n",
    "    ## PREPARE DATASET\n",
    "    ###################\n",
    "    ((x_train_multi, y_train_multi), (x_val_multi, y_val_multi)), mean_in, std_in, mean_out, std_out = \\\n",
    "                                    create_training_data(df, input_col, target_col, window_size=window_size,\n",
    "                                                        training_pattern_percent=training_pattern_percent)\n",
    "\n",
    "    print('trainData: Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "    print('trainData: Single window of future : {}'.format(y_train_multi[1].shape))\n",
    "    print('valData: Single window of past history : {}'.format(x_val_multi[0].shape))\n",
    "    print('valData: Single window of future : {}'.format(y_val_multi[1].shape))\n",
    "    print('trainData: number of trainingsexamples: {}'.format(x_train_multi.shape))\n",
    "    print('valData: number of trainingsexamples: {}'.format(x_val_multi.shape))\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    #train_data = train_data.cache().shuffle(max_training_pattern).batch(training_batch_size).repeat()\n",
    "    train_data = train_data.shuffle(x_train_multi.shape[0]).batch(training_batch_size).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data = val_data.batch(validation_batch_size).repeat()\n",
    "    input_shape = x_train_multi[0].shape[-2:]\n",
    "    return train_data, val_data, input_shape, mean_in, std_in, mean_out, std_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=4\n",
    "input_col = [COS_ANGLE, SIN_ANGLE, ANG_VEL, ACTION]\n",
    "target_col = [COS_ANGLE, SIN_ANGLE, ANG_VEL]\n",
    "\n",
    "train_data, val_data, input_shape, mean_in, std_in, mean_out, std_out =  \\\n",
    "            prepare_data(df, input_col, target_col, window_size=window_size, training_pattern_percent=0.7)\n",
    "\n",
    "print (\"Input-Shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, mean, std, **kwargs):\n",
    "        super(NormalizeLayer, self).__init__(**kwargs)\n",
    "        self.mean = tf.constant(mean, dtype=tf.float32)\n",
    "        self.std = tf.constant(std, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.mean) / self.std\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NormalizeLayer, self).get_config()\n",
    "        config.update({\n",
    "            'mean': self.mean.numpy().tolist(),\n",
    "            'std': self.std.numpy().tolist(),\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# Registrieren Sie Ihre benutzerdefinierte Schicht\n",
    "tf.keras.utils.get_custom_objects()['NormalizeLayer'] = NormalizeLayer\n",
    "\n",
    "def build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape, optimizer=tf.keras.optimizers.RMSprop()):\n",
    "\n",
    "    print(f\"mean in = {mean_in}, std in = {std_in}, mean out = {mean_out}, std out = {std_out}\")\n",
    "\n",
    "    single_step_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Use the custom Standardization layer\n",
    "    single_step_model.add(NormalizeLayer(mean=mean_in, std=std_in, input_shape=input_shape))\n",
    "    single_step_model.add(tf.keras.layers.LSTM(50, input_shape=input_shape, dtype=np.float32))\n",
    "    single_step_model.add(tf.keras.layers.Dense(len(mean_out), activation=\"linear\"))\n",
    "\n",
    "    single_step_model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return single_step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"model.keras\"\n",
    "max_epochs = 1000\n",
    "steps_per_epoch = 100\n",
    "validation_steps = 100\n",
    "validation_freq = 1\n",
    "\n",
    "# Callbacks\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True, verbose=True)\n",
    "mc_trainLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestTrainLoss.keras\" % modelpath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "mc_valLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestValLoss.keras\" % modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./model_logs_tb\", histogram_freq=1)\n",
    "\n",
    "# Modell erstellen\n",
    "step_model = build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape)\n",
    "\n",
    "# Modell trainieren\n",
    "history = step_model.fit(train_data, epochs=max_epochs, steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=val_data, validation_steps=validation_steps, validation_freq=validation_freq,\n",
    "                        callbacks=[mc_trainLoss_callback, mc_valLoss_callback, es_callback, tensorboard_callback])\n",
    "\n",
    "# Modell speichern\n",
    "step_model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEval = sample_data(episodes=1)\n",
    "#dfEval.describe()\n",
    "dfEval = dfEval[dfEval.episode==0]\n",
    "\n",
    "#row_max_steps = dfEval[dfEval.step == dfEval.step.max()]\n",
    "#dfEval = dfEval[dfEval.episode==int(row_max_steps.episode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min = y_train_multi.min(axis=0)\n",
    "output_max = y_train_multi.max(axis=0)\n",
    "print (\"min(output)_data: \", output_min)\n",
    "print (\"max(output)_data: \", output_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# load model\n",
    "#model = tf.keras.models.load_model(modelpath, compile=False)\n",
    "model = tf.keras.models.load_model(\"model.keras_bestValLoss.keras\", compile=False)\n",
    "#model = tf.keras.models.load_model(\"model.h5.bestTrainLoss\", compile=False)\n",
    "#############################################################################\n",
    "# in case of error: AttributeError: 'str' object has no attribute 'decode'\n",
    "# => Downgrade h5py package to version 2.10.0: pip install h5py==2.10.0\n",
    "#############################################################################\n",
    "\n",
    "# FIFO-buffer that keeps the neural state\n",
    "stateBuffer = collections.deque(maxlen=window_size)\n",
    "\n",
    "# outputs of neural network will be stored here\n",
    "transitions = []\n",
    "\n",
    "for i in range (len(dfEval)): \n",
    "                            \n",
    "    # estimation of first state\n",
    "    if i < window_size: \n",
    "        state_data = np.float32([dfEval[COS_ANGLE].values[i], dfEval[SIN_ANGLE].values[i],\n",
    "                               dfEval[ANG_VEL].values[i],\n",
    "                               dfEval[ACTION].values[i]])\n",
    "        stateBuffer.append(state_data)\n",
    "        #print (\"Filling initState: %s\" % state_data)\n",
    "    \n",
    "    # predict successor state\n",
    "    else: \n",
    "        \n",
    "        ###########################\n",
    "        # recall of neural network\n",
    "        ###########################\n",
    "        state = np.array([list(stateBuffer)])\n",
    "        if i==5:\n",
    "            print (state)\n",
    "        netOutput = model.predict(np.float32(state))[0]\n",
    "        \n",
    "        # clip output to observed data bounds\n",
    "        netOutput = np.clip(netOutput, output_min, output_max)\n",
    "        \n",
    "        # check if value bound was hit\n",
    "        if np.any(netOutput == output_min) or np.any(netOutput == output_max):\n",
    "            print (\"Bound-hit at step: \", i, \" => terminating further evaluation\")\n",
    "            break\n",
    "        \n",
    "        # append plotting data\n",
    "        transitions.append ({\n",
    "            COS_ANGLE:netOutput[0], SIN_ANGLE:netOutput[1],\n",
    "            ANG_VEL:netOutput[2]\n",
    "        })\n",
    "        \n",
    "        # update RNN state\n",
    "        stateBuffer.append(np.float32([netOutput[0], netOutput[1], \n",
    "                                       netOutput[2], \n",
    "                                       dfEval[ACTION].values[i]]))\n",
    "        \n",
    "dfNet = pd.DataFrame(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots (5, 1, figsize=(10,10))\n",
    "\n",
    "fields = [COS_ANGLE, SIN_ANGLE, ANG_VEL]\n",
    "\n",
    "for i in range (len(fields)):\n",
    "    f = fields[i]\n",
    "    axs[i].plot(range (len(dfNet)), dfEval[f].values[window_size:window_size+len(dfNet)], label=f)\n",
    "    axs[i].plot(range (len(dfNet)), dfNet[f].values, label=\"prediction\", ls=\"--\")\n",
    "    axs[i].grid()\n",
    "    axs[i].legend(loc=\"best\")\n",
    "    \n",
    "axs[4].plot(range (len(dfNet)), dfEval[ACTION].values[window_size:window_size+len(dfNet)], label=ACTION)\n",
    "axs[4].grid()\n",
    "axs[4].legend(loc=\"best\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
