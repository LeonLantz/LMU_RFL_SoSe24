{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/LMU_Muenchen_Logo.svg\" alt=\"Image\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "*Gruppe A - Thema 5*\n",
    "\n",
    "### **RL-Policy-Training unter Vermeidung von Zust√§nden mit sehr geringer Datendichte**\n",
    "\n",
    "- Erzeugung eines Datensatzes f√ºr das Absch√§tzung der Datendichte\n",
    "\n",
    "- Sampeln eines Gym-Environments \n",
    "    - Entfernen vordefinierter Bereiche, die z.B. durch die eine optimale Policy laufen w√ºrde\n",
    "\n",
    "- Modell-based RL\n",
    "\n",
    "- Datendichte im Reward abbilden, aber nicht √ºbergewichten\n",
    "\n",
    "**Dozent:** [Dr. Michel Tokic](https://www.tokic.com/)\n",
    "\n",
    "**Studenten:** Maximilian Schieder, Leon Lantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Konzept**\n",
    "\n",
    "Offline Reinforcement Learning!\n",
    "\n",
    "1. Sampling im Environment\n",
    "    - Ramdom oder gezielt (Heuristik, Rewards von Environment √§ndern)\n",
    "    - Zustand und entsprechende Aktion jedes Schrittes speichern\n",
    "2. Bestimmung der Datendichte\n",
    "3. Entfernung von Zust√§nden mit geringer Datendichte\n",
    "4. Verwendung des gefilterten Datensatzes f√ºr das Training\n",
    "5. Model-Based RFL ...\n",
    "\n",
    "Anstatt seltene Zust√§nde komplett zu entfernen, k√∂nnte man sie auch weniger stark gewichten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tensorflow` as `tf` for building and training machine learning models\n",
    "\n",
    "- `numpy` as `np` for numerical operations and handling arrays\n",
    "\n",
    "- `random` for generating random numbers and randomizing data\n",
    "\n",
    "- `gym` for creating and interacting with reinforcement learning environments (OpenAI)\n",
    "\n",
    "- `pandas` as `pd` for data manipulation and analysis\n",
    "\n",
    "- `matplotlib.colors` for color normalization in plots\n",
    "\n",
    "- `matplotlib.pyplot` as `plt` for creating visualizations and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the code is run, the random numbers generated will be the same, leading to **reproducible results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed (0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöó **Mountain Car**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/mountain_car.gif\" alt=\"Mountain Car GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Mountain Car MDP [**(Documentation)**](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "- **Goal**: Reach the goal state at the top of the right hill\n",
    "- **Problem**: The car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum\n",
    "- **Starting Position**: Car starts stochastically at the bottom of a valley\n",
    "\n",
    "#### **Observation Space**\n",
    "- 0: position of the car along the x-axis [-1.2, 0.6]\n",
    "- 1: velocity of the car [-0.07, 0.07]\n",
    "\n",
    "#### **Action Space**\n",
    "\n",
    "Discrete deterministic actions\n",
    "- 0: Accelerate to the left\n",
    "- 1: Don‚Äôt accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "#### **Reward**\n",
    "\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
    "\n",
    "#### **Episode End**\n",
    "- **Termination**: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "- **Truncation**: The length of the episode is 200.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "CAR_POS = \"carPos\"\n",
    "CAR_VEL = \"carVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "\n",
    "#OPTIONS: Set by User !!!\n",
    "RENDERED = True\n",
    "HEURISTIC = False\n",
    "\n",
    "\n",
    "def heuristic_policy(velocity):\n",
    "    if velocity >= 0.0:\n",
    "        return 2 # Accelerate right\n",
    "    else:\n",
    "        return 0 # Accelerate left\n",
    "\n",
    "\n",
    "def sample_data(episodes=1, seed=0):\n",
    "\n",
    "    if(RENDERED): env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "    else: env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    env.reset()\n",
    "\n",
    "    ### Create empty Pandas dataset\n",
    "    transitions = []\n",
    "\n",
    "    ### SAMPLE DATA\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while step < 200 and not done:\n",
    "            step += 1\n",
    "\n",
    "            if(step == 1):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                if (HEURISTIC): action = heuristic_policy(obs[1])\n",
    "                else: action = env.action_space.sample()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            transitions.append(\n",
    "                {\n",
    "                    CAR_POS: obs[0],\n",
    "                    CAR_VEL: obs[1],\n",
    "                    EPISODE: episode,\n",
    "                    STEP: step,\n",
    "                    ACTION: action,\n",
    "                }\n",
    "            )\n",
    "        print(\"Steps: \", step)\n",
    "\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "df = sample_data(episodes=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderer Ansatz:**\n",
    "\n",
    "-> Reward √§ndern. z.B. wenn Car halbwegs gute Ergebnisse erzielt (weit oben auf H√ºgel) Reward +1 setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: red;\">Zust√§nde nahe des Gipfels sind <b>selten, aber wichtig</b> um das Auto in den Gipfelbereich zu bringen. Das Vermeiden solcher Zust√§nde k√∂nnte dazu f√ºhren, dass das Modell nicht lernt, wie man erfolgreich den Gipfel erreicht.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåë **Lunar Lander**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gymlibrary.dev/_images/lunar_lander.gif\" alt=\"Lunar Lander GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Lunar Lander [**(Documentation)**](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "\n",
    "- **Ziel**: Das Ziel besteht darin, den Lander sicher auf dem Landepad zu landen.\n",
    "\n",
    "- **Ausgangsposition**: Der Lander startet in der oberen Mitte des Ansichtsbereichs mit einer zuf√§lligen Anfangskraft, die auf seinen Schwerpunkt angewendet wird.\n",
    "\n",
    "#### **Beobachtungsraum**\n",
    "Der Beobachtungsraum ist ein 8-dimensionaler Vektor\n",
    "- [0] : x-Koordinate des Landers\n",
    "- [1] : y-Koordinate des Landers\n",
    "- [2] : lineare Geschwindigkeit in x-Richtung\n",
    "- [3] : lineare Geschwindigkeit in y-Richtung\n",
    "- [4] : Winkel\n",
    "- [5] : Winkelgeschwindigkeit\n",
    "- [6] : linkes Bein in Kontakt mit dem Boden (Boolean)\n",
    "- [7] : rechtes Bein in Kontakt mit dem Boden (Boolean)\n",
    "\n",
    "#### **Aktionsraum**\n",
    "Vier diskrete Aktionen\n",
    "- [0] : nichts tun\n",
    "- [1] : linke Orientierungsturbine feuern\n",
    "- [2] : Hauptturbine feuern\n",
    "- [3] : rechte Orientierungsturbine feuern\n",
    "\n",
    "#### **Belohnung**\n",
    "- Die Belohnung erh√∂ht/verringert sich, je n√§her/weiter der Lander dem Landepad ist.\n",
    "- Die Belohnung erh√∂ht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Die Belohnung verringert sich, je mehr der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Die Belohnung erh√∂ht sich um 10 Punkte f√ºr jedes Bein, das den Boden ber√ºhrt.\n",
    "- Die Belohnung verringert sich um 0,03 Punkte f√ºr jeden Frame, in dem eine Seitenturbine feuert.\n",
    "- Die Belohnung verringert sich um 0,3 Punkte f√ºr jeden Frame, in dem die Hauptturbine feuert.\n",
    "\n",
    "#### **Episodenbeendigung**\n",
    "- Der Lander st√ºrzt ab (der Landerk√∂rper kommt mit dem Mond in Kontakt).\n",
    "- Der Lander ger√§t au√üerhalb des Ansichtsbereichs (x-Koordinate ist gr√∂√üer als 1).\n",
    "- Der Lander ist nicht wach. Laut den Box2D-Dokumenten ist ein K√∂rper, der nicht wach ist, ein K√∂rper, der sich nicht bewegt und mit keinem anderen K√∂rper kollidiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Environment Name\n",
    "environment_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåé **Zuf√§llige Action-Auswahl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate(env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        while not done and not terminated:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "env = gym.make(environment_name)\n",
    "returns = evaluate(env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/random/episode_{episode}.gif\"\n",
    "    state, info = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Zuf√§llige Auswahl einer Aktion\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, terminated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:lightgreen;\">Ergebnis:</p>  Bei der Durchf√ºhrung von zehn Episoden in der <b>Lunar Lander-Umgebung</b>, in denen der Agent zuf√§llig Aktionen ausw√§hlt, waren die Ergebnisse erwartungsgem√§√ü schlecht. Der kumulierte Reward ist in der Regel negativ, da zuf√§llige Aktionen selten zu einer erfolgreichen Landung f√ºhren. Diese Ergebnisse zeigen deutlich, wie wichtig durchdachte Strategien und gut trainierte Modelle f√ºr den Erfolg in komplexen Umgebungen wie Lunar Lander sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãüèº **Trainiere ein Modell mit dem A2C-Algorithmus** <b style=\"color:red;\">(Optional)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = A2C('MlpPolicy', env, ent_coef=0.1, verbose=1)\n",
    "model.learn(total_timesteps=80000)\n",
    "model.save(\"models/A2C_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üë®‚Äçüè´ **Bewertung der Modelle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#del model #optional\n",
    "#model = A2C.load('models/A2C_model', env=env)\n",
    "model2 = A2C.load('models/a2c-LunarLander-v2')\n",
    "# Andere Modelle zur Datengenerierung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchschnitt √ºber 100 Episoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "returns = evaluate_model(model2, env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichere 10 Episoden als GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/pretrained_model/episode_{episode}.gif\"\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Vorhersage der Aktion durch das Modell\n",
    "        action, _states = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "# Schlie√üe das Environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø **Data Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Action Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten f√ºr die Spaltennamen\n",
    "X_POS = \"xPos\"\n",
    "Y_POS = \"yPos\"\n",
    "X_VEL = \"xVel\"\n",
    "Y_VEL = \"yVel\"\n",
    "ANGLE = \"angle\"\n",
    "ANGULAR_VEL = \"angularVel\"\n",
    "LEFT_LEG_CONTACT = \"leftLegContact\"\n",
    "RIGHT_LEG_CONTACT = \"rightLegContact\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "def sample_data(episodes=10000):\n",
    "    # Erstelle die LunarLander-Umgebung\n",
    "    env = gym.make(environment_name)\n",
    "    \n",
    "    # Erstelle eine leere Liste zur Speicherung der √úberg√§nge\n",
    "    transitions = []\n",
    "\n",
    "    # Sammle Daten aus der Umgebung\n",
    "    for episode in range(episodes):\n",
    "        # Setze die Umgebung zur√ºck und erhalte die Anfangsbeobachtung\n",
    "        obs, info = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        terminated = False\n",
    "\n",
    "        while not done and not terminated:\n",
    "            step += 1\n",
    "            # W√§hle eine zuf√§llige Aktion\n",
    "            action, _states = model2.predict(obs)\n",
    "            print(action, _states)\n",
    "\n",
    "            # F√ºge den aktuellen Zustand und die Aktion zur Liste der √úberg√§nge hinzu\n",
    "            transitions.append({\n",
    "                X_POS: obs[0], \n",
    "                Y_POS: obs[1], \n",
    "                X_VEL: obs[2], \n",
    "                Y_VEL: obs[3],\n",
    "                ANGLE: obs[4], \n",
    "                ANGULAR_VEL: obs[5], \n",
    "                LEFT_LEG_CONTACT: int(obs[6]), \n",
    "                RIGHT_LEG_CONTACT: int(obs[7]),\n",
    "                EPISODE: episode, \n",
    "                STEP: step, \n",
    "                ACTION: int(action)\n",
    "            })\n",
    "\n",
    "            # F√ºhre die Aktion aus und erhalte den n√§chsten Zustand und die Belohnung\n",
    "            obs, reward, done, terminated, _ = env.step(action)\n",
    "            if(terminated):\n",
    "                print(\"Halllo\")\n",
    "\n",
    "    # Konvertiere die Liste der √úberg√§nge in ein Pandas DataFrame\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "# Sammle Daten aus der LunarLander-v2 Umgebung\n",
    "df = sample_data(episodes=200)\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Action Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanten f√ºr die Spaltennamen\n",
    "X_POS = \"xPos\"\n",
    "Y_POS = \"yPos\"\n",
    "X_VEL = \"xVel\"\n",
    "Y_VEL = \"yVel\"\n",
    "ANGLE = \"angle\"\n",
    "ANGULAR_VEL = \"angularVel\"\n",
    "LEFT_LEG_CONTACT = \"leftLegContact\"\n",
    "RIGHT_LEG_CONTACT = \"rightLegContact\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "def sample_data(episodes=10000):\n",
    "    # Erstelle die LunarLander-Umgebung\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    # Erstelle eine leere Liste zur Speicherung der √úberg√§nge\n",
    "    transitions = []\n",
    "\n",
    "    # Sammle Daten aus der Umgebung\n",
    "    for episode in range(episodes):\n",
    "        # Setze die Umgebung zur√ºck und erhalte die Anfangsbeobachtung\n",
    "        obs, info = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            # W√§hle eine zuf√§llige Aktion\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # F√ºge den aktuellen Zustand und die Aktion zur Liste der √úberg√§nge hinzu\n",
    "            transitions.append({\n",
    "                X_POS: obs[0], \n",
    "                Y_POS: obs[1], \n",
    "                X_VEL: obs[2], \n",
    "                Y_VEL: obs[3],\n",
    "                ANGLE: obs[4], \n",
    "                ANGULAR_VEL: obs[5], \n",
    "                LEFT_LEG_CONTACT: int(obs[6]), \n",
    "                RIGHT_LEG_CONTACT: int(obs[7]),\n",
    "                EPISODE: episode, \n",
    "                STEP: step, \n",
    "                ACTION: action\n",
    "            })\n",
    "\n",
    "            # F√ºhre die Aktion aus und erhalte den n√§chsten Zustand und die Belohnung\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            done = done or truncated\n",
    "\n",
    "    # Konvertiere die Liste der √úberg√§nge in ein Pandas DataFrame\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "# Sammle Daten aus der LunarLander-v2 Umgebung\n",
    "df = sample_data(episodes=2000)\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a 2D histogram of xPos vs yPos\n",
    "plt.figure(figsize=(10, 7))\n",
    "h = plt.hist2d(df[X_POS], df[Y_POS], bins=40, norm=LogNorm(), cmap='Reds')\n",
    "plt.colorbar(h[3])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.title(\"2D Histogram of X Position vs Y Position\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10,15), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è **Cut Out Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Data density**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è **Model Based RFL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(data, input_col, target_col, window_size=1, training_pattern_percent=0.7):\n",
    "    \n",
    "    data_train = data\n",
    "\n",
    "    mean_in, std_in = mean_and_std(input_col, data_train)\n",
    "    mean_out, std_out = mean_and_std(target_col, data_train)\n",
    "    \n",
    "    print(f\"mean in = {mean_in}\" )\n",
    "    print(f\"std in = {std_in}\")\n",
    "    print(f\"mean out =  {mean_out}\")\n",
    "    print(f\"std out = {std_out}\")\n",
    "\n",
    "    grouped = data_train.groupby(['episode'])\n",
    "\n",
    "    inputs_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    for g in grouped:\n",
    "        g = g[1].sort_values(by='step')\n",
    "\n",
    "        past_history = window_size\n",
    "        future_target = 0\n",
    "        STEP = 1\n",
    "\n",
    "        inputs, labels = multivariate_data(\n",
    "            dataset=g[input_col][:].values,\n",
    "            target=g[target_col][:].values,\n",
    "            start_index=0,\n",
    "            end_index=g[input_col][:].values.shape[0] - future_target,\n",
    "            history_size=past_history,\n",
    "            target_size=future_target,\n",
    "            step=STEP,\n",
    "            single_step=True\n",
    "        )\n",
    "\n",
    "        for i in range(len(inputs)):\n",
    "            inputs_all.append(inputs[i])\n",
    "            labels_all.append(labels[i])\n",
    "  \n",
    "    length = len(inputs_all)\n",
    "\n",
    "    c = list(zip(inputs_all, labels_all))\n",
    "    np.random.shuffle(c)\n",
    "    inputs_all, labels_all = zip(*c)\n",
    "\n",
    "    split = int(training_pattern_percent * length)\n",
    "\n",
    "    inputs_all = np.array(inputs_all)\n",
    "    labels_all = np.array(labels_all)\n",
    "\n",
    "    return ((inputs_all[:split], labels_all[:split]), (inputs_all[split:], labels_all[split:])), mean_in, std_in, mean_out, std_out\n",
    "\n",
    "def mean_and_std(columns, data):\n",
    "    mean = np.zeros(len(columns))\n",
    "    std = np.zeros(len(columns))\n",
    "    for index, c in enumerate(columns):\n",
    "        mean[index], std[index] = get_normalizations(data[c])\n",
    "    return mean, std\n",
    "\n",
    "def get_normalizations(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return mean, std\n",
    "\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "       end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i + target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i + target_size])\n",
    "\n",
    "    return np.array(data, dtype=np.float32), np.array(labels, dtype=np.float32)\n",
    "\n",
    "def prepare_data(df, input_col, target_col, window_size, training_batch_size=50, validation_batch_size=50, training_pattern_percent=0.7):\n",
    "    \n",
    "    global x_train_multi, y_train_multi\n",
    "    \n",
    "    ((x_train_multi, y_train_multi), (x_val_multi, y_val_multi)), mean_in, std_in, mean_out, std_out = \\\n",
    "                                    create_training_data(df, input_col, target_col, window_size=window_size,\n",
    "                                                        training_pattern_percent=training_pattern_percent)\n",
    "    \n",
    "    print(\"----\")\n",
    "    print(x_train_multi[0])\n",
    "    print(\"----\")\n",
    "    print(y_train_multi[0])\n",
    "    print(\"----\")\n",
    "\n",
    "    print('trainData: Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "    print('trainData: Single window of future : {}'.format(y_train_multi[1].shape))\n",
    "    print('valData: Single window of past history : {}'.format(x_val_multi[0].shape))\n",
    "    print('valData: Single window of future : {}'.format(y_val_multi[1].shape))\n",
    "    print('trainData: number of training examples: {}'.format(x_train_multi.shape))\n",
    "    print('valData: number of training examples: {}'.format(x_val_multi.shape))\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "    train_data = train_data.shuffle(x_train_multi.shape[0]).batch(training_batch_size).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "    val_data = val_data.batch(validation_batch_size).repeat()\n",
    "    input_shape = x_train_multi[0].shape[-2:]\n",
    "    print(\"InputShape: \", input_shape)\n",
    "    return train_data, val_data, input_shape, mean_in, std_in, mean_out, std_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "input_col = [X_POS, X_VEL, Y_POS, Y_VEL, ANGLE, ANGULAR_VEL, LEFT_LEG_CONTACT, RIGHT_LEG_CONTACT, ACTION]\n",
    "target_col = [X_POS, X_VEL, Y_POS, Y_VEL, ANGLE, ANGULAR_VEL, LEFT_LEG_CONTACT, RIGHT_LEG_CONTACT]\n",
    "\n",
    "# Vorbereitung der Daten\n",
    "train_data, val_data, input_shape, mean_in, std_in, mean_out, std_out = prepare_data(\n",
    "    df, input_col, target_col, window_size=window_size, training_pattern_percent=0.7\n",
    ")\n",
    "\n",
    "print(\"Input-Shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "for i, (X, y) in enumerate(train_data):\n",
    "    print(f\"Element {i+1}:\")\n",
    "    print(\"X (Input):\")\n",
    "    print(X.numpy())\n",
    "    print(\"y (Target):\")\n",
    "    print(y.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Lunar-Lander state-transition model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, mean, std, **kwargs):\n",
    "        super(NormalizeLayer, self).__init__(**kwargs)\n",
    "        self.mean = tf.constant(mean, dtype=tf.float32)\n",
    "        self.std = tf.constant(std, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.mean) / self.std\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NormalizeLayer, self).get_config()\n",
    "        config.update({\n",
    "            'mean': self.mean.numpy().tolist(),\n",
    "            'std': self.std.numpy().tolist(),\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Registrieren Sie Ihre benutzerdefinierte Schicht\n",
    "tf.keras.utils.get_custom_objects()['NormalizeLayer'] = NormalizeLayer\n",
    "\n",
    "def build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape, optimizer=tf.keras.optimizers.RMSprop()):\n",
    "    print(f\"mean = {mean_in}, std = {std_in}, mean = {mean_out}, std = {std_out}\")\n",
    "    single_step_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Ersetzen Sie den Lambda-Layer durch den benutzerdefinierten NormalizeLayer\n",
    "    # Erh√∂hen Sie die Modellkapazit√§t mit zus√§tzlichen LSTM- und Dense-Schichten\n",
    "    single_step_model.add(tf.keras.layers.LSTM(150, return_sequences=True, dtype=np.float32))\n",
    "    single_step_model.add(tf.keras.layers.LSTM(100, dtype=np.float32))\n",
    "    single_step_model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
    "    single_step_model.add(tf.keras.layers.Dense(len(mean_out), activation=\"linear\"))\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)  # Niedrigere Lernrate\n",
    "    single_step_model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return single_step_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"model.keras\"\n",
    "max_epochs = 1000\n",
    "steps_per_epoch = 500\n",
    "validation_steps = 100\n",
    "validation_freq = 1\n",
    "\n",
    "# Callbacks\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=True, verbose=True)\n",
    "mc_trainLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestTrainLoss.keras\" % modelpath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "mc_valLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestValLoss.keras\" % modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./model_logs_tb\", histogram_freq=1)\n",
    "\n",
    "# Modell erstellen\n",
    "step_model = build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape)\n",
    "\n",
    "# Modell trainieren\n",
    "history = step_model.fit(train_data, epochs=max_epochs, steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=val_data, validation_steps=validation_steps, validation_freq=validation_freq,\n",
    "                        callbacks=[mc_trainLoss_callback, mc_valLoss_callback, es_callback, tensorboard_callback])\n",
    "\n",
    "print(history)\n",
    "\n",
    "# Modell speichern\n",
    "step_model.save(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEval = sample_data(episodes=1)\n",
    "#dfEval.describe()\n",
    "dfEval = dfEval[dfEval.episode==0]\n",
    "\n",
    "#row_max_steps = dfEval[dfEval.step == dfEval.step.max()]\n",
    "#dfEval = dfEval[dfEval.episode==int(row_max_steps.episode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min = y_train_multi.min(axis=0)\n",
    "output_max = y_train_multi.max(axis=0)\n",
    "print (\"min(output)_data: \", output_min)\n",
    "print (\"max(output)_data: \", output_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Aktivieren Sie unsichere Deserialisierung, wenn Sie dem Modell vertrauen\n",
    "#tf.keras.config.enable_unsafe_deserialization()\n",
    "\n",
    "# Modellpfad (Stellen Sie sicher, dass dies den richtigen Pfad und Dateinamen hat)\n",
    "modelpath = \"model.keras_bestValLoss.keras\"\n",
    "\n",
    "# Modell laden\n",
    "model = tf.keras.models.load_model(modelpath, compile=False)\n",
    "\n",
    "# FIFO-Puffer, der den Zustand des neuronalen Netzwerks speichert\n",
    "stateBuffer = collections.deque(maxlen=window_size)\n",
    "\n",
    "# Hier werden die Ausgaben des neuronalen Netzwerks gespeichert\n",
    "transitions = []\n",
    "\n",
    "# Annahme: dfEval ist ein DataFrame, das die Evaluierungsdaten enth√§lt\n",
    "for i in range(len(dfEval)): \n",
    "\n",
    "    \n",
    "                            \n",
    "    # Initialisierung des ersten Zustands\n",
    "    if i < window_size: \n",
    "        state_data = np.float32([dfEval[X_POS].values[i], dfEval[Y_POS].values[i],\n",
    "                               dfEval[X_VEL].values[i], dfEval[Y_VEL].values[i],\n",
    "                               dfEval[ANGLE].values[i], dfEval[ANGULAR_VEL].values[i],\n",
    "                               dfEval[LEFT_LEG_CONTACT].values[i], dfEval[RIGHT_LEG_CONTACT].values[i],\n",
    "                               dfEval[ACTION].values[i]])\n",
    "        stateBuffer.append(state_data)\n",
    "        #print(\"Filling initState: %s\" % state_data)\n",
    "    \n",
    "    # Vorhersage des Nachfolgezustands\n",
    "    else: \n",
    "        \n",
    "        ###########################\n",
    "        # Abrufen der Vorhersage vom neuronalen Netzwerk\n",
    "        ###########################\n",
    "        # Vorhersage des Nachfolgezustands\n",
    "        state = np.array([list(stateBuffer)])\n",
    "\n",
    "        print(state)\n",
    "\n",
    "        netOutput = model.predict(np.float32(state))[0]\n",
    "        print(netOutput)\n",
    "        \n",
    "        if i == 5:\n",
    "            print(state)\n",
    "        netOutput = model.predict(np.float32(state))[0]\n",
    "        print(f\"Predicted Output: {netOutput}\")\n",
    "\n",
    "\n",
    "        # Ausgabe auf die Beobachtungsdaten beschr√§nken\n",
    "        netOutput = np.clip(netOutput, output_min, output_max)\n",
    "        \n",
    "        # √úberpr√ºfen, ob der Wert die Grenze erreicht hat\n",
    "        #if np.any(netOutput == output_min) or np.any(netOutput == output_max):\n",
    "        #    print(\"Bound-hit at step: \", i, \" => terminating further evaluation\")\n",
    "        #    break\n",
    "        \n",
    "        # Hinzuf√ºgen von Plot-Daten\n",
    "        transitions.append({\n",
    "            X_POS: netOutput[0], Y_POS: netOutput[1],\n",
    "            X_VEL: netOutput[2], Y_VEL: netOutput[3],\n",
    "            ANGLE: netOutput[4], ANGULAR_VEL: netOutput[5],\n",
    "            LEFT_LEG_CONTACT: netOutput[6], RIGHT_LEG_CONTACT: netOutput[7]\n",
    "        })\n",
    "        \n",
    "        # Aktualisieren des RNN-Zustands\n",
    "        stateBuffer.append(np.float32([netOutput[0], netOutput[1], \n",
    "                                       netOutput[2], netOutput[3],\n",
    "                                       netOutput[4], netOutput[5],\n",
    "                                       netOutput[6], netOutput[7],\n",
    "                                       dfEval[ACTION].values[i]]))\n",
    "        \n",
    "# Erstellen des DataFrames aus den Vorhersagen\n",
    "dfNet = pd.DataFrame(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispielhafte Datenfelder f√ºr LunarLander\n",
    "fields = [X_POS, Y_POS, X_VEL, Y_VEL, ANGLE, ANGULAR_VEL, LEFT_LEG_CONTACT, RIGHT_LEG_CONTACT]\n",
    "\n",
    "# Plot erstellen\n",
    "fig, axs = plt.subplots(8, 1, figsize=(12, 20))\n",
    "\n",
    "# F√ºr jede der relevanten Beobachtungen plotten\n",
    "for i in range(len(fields)):\n",
    "    f = fields[i]\n",
    "    if f in dfEval.columns and f in dfNet.columns:\n",
    "        axs[i].plot(range(len(dfNet)), dfEval[f].values[window_size:window_size+len(dfNet)], label=f\"True {f}\")\n",
    "        axs[i].plot(range(len(dfNet)), dfNet[f].values, label=f\"Predicted {f}\", ls=\"--\")\n",
    "        axs[i].grid()\n",
    "        axs[i].legend(loc=\"best\")\n",
    "    else:\n",
    "        axs[i].plot([], [], label=f\"Field {f} not available\")\n",
    "        axs[i].grid()\n",
    "        axs[i].legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "        self.fc = nn.Linear(model_dim, output_dim)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = input_data.shape[1]\n",
    "model_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "output_dim = output_data.shape[0]\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for src, tgt in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Assuming we use src and tgt as both input and target here\n",
    "        output = model(src, src)\n",
    "        loss = criterion(output.squeeze(0), tgt.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "print('Training complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoren**: Maximilian Schieder, Leon Lantz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
