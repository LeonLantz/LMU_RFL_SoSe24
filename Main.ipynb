{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "*Gruppe A - Thema 5*\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/LMU_Muenchen_Logo.svg\" alt=\"Image\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "### **RL-Policy-Training unter Vermeidung von Zust√§nden mit sehr geringer Datendichte**\n",
    "\n",
    "\n",
    "**Dozent:** [Dr. Michel Tokic](https://www.tokic.com/)\n",
    "\n",
    "**Studenten:** Maximilian Schieder, Leon Lantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aufgabenstellung**\n",
    "\n",
    "- Erzeugung eines Datensatzes f√ºr das Absch√§tzung der Datendichte\n",
    "\n",
    "- Sampeln eines Gym-Environments \n",
    "    - Entfernen vordefinierter Bereiche, die z.B. durch die eine optimale Policy laufen w√ºrde\n",
    "\n",
    "- Modell-based RL\n",
    "\n",
    "- Datendichte im Reward abbilden, aber nicht √ºbergewichten\n",
    "\n",
    "## **Konzept**\n",
    "\n",
    "1. **Environment ausw√§hlen und verstehen** ‚úÖ\n",
    "    - Pendulum-v1\n",
    "2. **Sampling im Environment** ‚úÖ\n",
    "    - Zustand und entsprechende Aktion jedes Schrittes speichern\n",
    "3. **Datenbereich herausschneiden** ‚úÖ\n",
    "    - 60-120 Grad (**Ziel**: *Pendel nimmt immer den linken Weg*)\n",
    "4. **Datendichtemodell erstellen** ‚úÖ\n",
    "    - Kernel Density Estimation \n",
    "5. **Model-Based RFL** ‚úÖ\n",
    "    - Window-Size 4, Adaptive Moment Estimation(Adam)-Optimizer\n",
    "6. **Lernen der Policy mit DDPG** üîß\n",
    "    - Modifizierte Reward Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ **Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tensorflow` als `tf` zum Erstellen und Trainieren von maschinellen Lernmodellen\n",
    "\n",
    "- `numpy` als `np` f√ºr numerische Operationen und den Umgang mit Arrays\n",
    "\n",
    "- `gymnasium` als `gym` zum Erstellen und Interagieren mit Reinforcement Learning-Umgebungen\n",
    "\n",
    "- `pandas` als `pd` f√ºr Datenmanipulation und -analyse\n",
    "\n",
    "- `matplotlib.pyplot` als `plt` zum Erstellen von Visualisierungen und Diagrammen\n",
    "\n",
    "- `stable_baselines3.PPO` f√ºr Proximal Policy Optimization, einen Reinforcement Learning-Algorithmus\n",
    "\n",
    "- `stable_baselines3.common.vec_env.DummyVecEnv` zum Erstellen einer Dummy-Vektorisierungsumgebung\n",
    "\n",
    "- `stable_baselines3.common.vec_env.VecNormalize` zum Normalisieren von Beobachtungen und Belohnungen\n",
    "\n",
    "- `stable_baselines3.common.monitor.Monitor` zur √úberwachung der Trainingsleistung im Reinforcement Learning\n",
    "\n",
    "- `stable_baselines3.common.results_plotter.load_results` zum Laden von Trainingsergebnissen im Reinforcement Learning\n",
    "\n",
    "- `stable_baselines3.common.results_plotter.ts2xy` zum Konvertieren von Trainingsergebnissen in Zeitstufen und Werte\n",
    "\n",
    "- `stable_baselines3.common` f√ºr allgemeine Hilfsprogramme und Funktionen in Stable Baselines3\n",
    "\n",
    "- `os` zum Interagieren mit Betriebssystemfunktionen wie Datei- und Verzeichnisverwaltung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "import collections\n",
    "import os\n",
    "\n",
    "#for custom environment\n",
    "from typing import Optional\n",
    "from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "# Importiere die Funktionen aus der utils.py Datei\n",
    "from utils import SaveOnBestTrainingRewardCallback, create_training_data\n",
    "from custom_environment import Pendulum_Custom_Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, mean, std, **kwargs):\n",
    "        super(NormalizeLayer, self).__init__(**kwargs)\n",
    "        self.mean = tf.constant(mean, dtype=tf.float32)\n",
    "        self.std = tf.constant(std, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.mean) / self.std\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NormalizeLayer, self).get_config()\n",
    "        config.update({\n",
    "            'mean': self.mean.numpy().tolist(),\n",
    "            'std': self.std.numpy().tolist(),\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "# Registrieren Sie Ihre benutzerdefinierte Schicht\n",
    "tf.keras.utils.get_custom_objects()['NormalizeLayer'] = NormalizeLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafikkarte verf√ºgbar?\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: red; font-weight: bold;\">(Deprecated)</span> üöó **Mountain Car**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/mountain_car.gif\" alt=\"Mountain Car GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Mountain Car MDP [**(Documentation)**](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "- **Goal**: Reach the goal state at the top of the right hill\n",
    "- **Problem**: The car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum\n",
    "- **Starting Position**: Car starts stochastically at the bottom of a valley\n",
    "\n",
    "#### **Observation Space**\n",
    "- 0: position of the car along the x-axis [-1.2, 0.6]\n",
    "- 1: velocity of the car [-0.07, 0.07]\n",
    "\n",
    "#### **Action Space**\n",
    "\n",
    "Discrete deterministic actions\n",
    "- 0: Accelerate to the left\n",
    "- 1: Don‚Äôt accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "#### **Reward**\n",
    "\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
    "\n",
    "#### **Episode End**\n",
    "- **Termination**: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "- **Truncation**: The length of the episode is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "CAR_POS = \"carPos\"\n",
    "CAR_VEL = \"carVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "\n",
    "#OPTIONS: Set by User !!!\n",
    "RENDERED = True\n",
    "HEURISTIC = False\n",
    "\n",
    "\n",
    "def heuristic_policy(velocity):\n",
    "    if velocity >= 0.0:\n",
    "        return 2 # Accelerate right\n",
    "    else:\n",
    "        return 0 # Accelerate left\n",
    "\n",
    "\n",
    "def sample_data(episodes=1, seed=0):\n",
    "\n",
    "    if(RENDERED): env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "    else: env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    env.reset()\n",
    "\n",
    "    ### Create empty Pandas dataset\n",
    "    transitions = []\n",
    "\n",
    "    ### SAMPLE DATA\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while step < 200 and not done:\n",
    "            step += 1\n",
    "\n",
    "            if(step == 1):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                if (HEURISTIC): action = heuristic_policy(obs[1])\n",
    "                else: action = env.action_space.sample()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            transitions.append(\n",
    "                {\n",
    "                    CAR_POS: obs[0],\n",
    "                    CAR_VEL: obs[1],\n",
    "                    EPISODE: episode,\n",
    "                    STEP: step,\n",
    "                    ACTION: action,\n",
    "                }\n",
    "            )\n",
    "        print(\"Steps: \", step)\n",
    "\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "df = sample_data(episodes=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderer Ansatz:**\n",
    "\n",
    "-> Reward √§ndern. z.B. wenn Car halbwegs gute Ergebnisse erzielt (weit oben auf H√ºgel) Reward +1 setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: red;\">Zust√§nde nahe des Gipfels sind <b>selten, aber wichtig</b> um das Auto in den Gipfelbereich zu bringen. Das Vermeiden solcher Zust√§nde k√∂nnte dazu f√ºhren, dass das Modell nicht lernt, wie man erfolgreich den Gipfel erreicht.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: red; font-weight: bold;\">(Deprecated)</span> üåë **Lunar Lander**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gymlibrary.dev/_images/lunar_lander.gif\" alt=\"Lunar Lander GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Lunar Lander [**(Documentation)**](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "\n",
    "- **Ziel**: Das Ziel besteht darin, den Lander sicher auf dem Landepad zu landen.\n",
    "\n",
    "- **Ausgangsposition**: Der Lander startet in der oberen Mitte des Ansichtsbereichs mit einer zuf√§lligen Anfangskraft, die auf seinen Schwerpunkt angewendet wird.\n",
    "\n",
    "#### **Beobachtungsraum**\n",
    "Der Beobachtungsraum ist ein 8-dimensionaler Vektor\n",
    "- [0] : x-Koordinate des Landers\n",
    "- [1] : y-Koordinate des Landers\n",
    "- [2] : lineare Geschwindigkeit in x-Richtung\n",
    "- [3] : lineare Geschwindigkeit in y-Richtung\n",
    "- [4] : Winkel\n",
    "- [5] : Winkelgeschwindigkeit\n",
    "- [6] : linkes Bein in Kontakt mit dem Boden (Boolean)\n",
    "- [7] : rechtes Bein in Kontakt mit dem Boden (Boolean)\n",
    "\n",
    "#### **Aktionsraum**\n",
    "Vier diskrete Aktionen\n",
    "- [0] : nichts tun\n",
    "- [1] : linke Orientierungsturbine feuern\n",
    "- [2] : Hauptturbine feuern\n",
    "- [3] : rechte Orientierungsturbine feuern\n",
    "\n",
    "#### **Belohnung**\n",
    "- Die Belohnung erh√∂ht/verringert sich, je n√§her/weiter der Lander dem Landepad ist.\n",
    "- Die Belohnung erh√∂ht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Die Belohnung verringert sich, je mehr der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Die Belohnung erh√∂ht sich um 10 Punkte f√ºr jedes Bein, das den Boden ber√ºhrt.\n",
    "- Die Belohnung verringert sich um 0,03 Punkte f√ºr jeden Frame, in dem eine Seitenturbine feuert.\n",
    "- Die Belohnung verringert sich um 0,3 Punkte f√ºr jeden Frame, in dem die Hauptturbine feuert.\n",
    "\n",
    "#### **Episodenbeendigung**\n",
    "- Der Lander st√ºrzt ab (der Landerk√∂rper kommt mit dem Mond in Kontakt).\n",
    "- Der Lander ger√§t au√üerhalb des Ansichtsbereichs (x-Koordinate ist gr√∂√üer als 1).\n",
    "- Der Lander ist nicht wach. Laut den Box2D-Dokumenten ist ein K√∂rper, der nicht wach ist, ein K√∂rper, der sich nicht bewegt und mit keinem anderen K√∂rper kollidiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Environment Name\n",
    "environment_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåé **Zuf√§llige Action-Auswahl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate(env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        while not done and not terminated:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "env = gym.make(environment_name)\n",
    "returns = evaluate(env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/random/episode_{episode}.gif\"\n",
    "    state, info = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Zuf√§llige Auswahl einer Aktion\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, terminated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:lightgreen;\">Ergebnis:</p>  Bei der Durchf√ºhrung von zehn Episoden in der <b>Lunar Lander-Umgebung</b>, in denen der Agent zuf√§llig Aktionen ausw√§hlt, waren die Ergebnisse erwartungsgem√§√ü schlecht. Der kumulierte Reward ist in der Regel negativ, da zuf√§llige Aktionen selten zu einer erfolgreichen Landung f√ºhren. Diese Ergebnisse zeigen deutlich, wie wichtig durchdachte Strategien und gut trainierte Modelle f√ºr den Erfolg in komplexen Umgebungen wie Lunar Lander sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãüèº **Trainiere ein Modell mit dem A2C-Algorithmus** <b style=\"color:red;\">(Optional)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = A2C('MlpPolicy', env, ent_coef=0.1, verbose=1)\n",
    "model.learn(total_timesteps=80000)\n",
    "model.save(\"models/A2C_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üë®‚Äçüè´ **Bewertung der Modelle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#del model #optional\n",
    "#model = A2C.load('models/A2C_model', env=env)\n",
    "model2 = A2C.load('models/a2c-LunarLander-v2')\n",
    "# Andere Modelle zur Datengenerierung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchschnitt √ºber 100 Episoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "returns = evaluate_model(model2, env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichere 10 Episoden als GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/pretrained_model/episode_{episode}.gif\"\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Vorhersage der Aktion durch das Modell\n",
    "        action, _states = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "# Schlie√üe das Environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÇ **Pendulum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gymlibrary.dev/_images/pendulum.gif\" alt=\"Mountain Car GIF\" width=\"300\">\n",
    "    \n",
    "\n",
    "Gym Umgebung -> Pendulum [(Dokumentation)](https://www.gymlibrary.dev/environments/classic_control/pendulum/)  \n",
    "- **Ziel** : Das Ziel besteht darin, das Pendel aufrecht zu halten (nach oben gerichtet).\n",
    " \n",
    "- **Ausgangsposition** : Das Pendel startet mit einer zuf√§lligen Position und Geschwindigkeit, wobei es meistens nach unten h√§ngt.\n",
    "\n",
    "**Beobachtungsraum** \n",
    "\n",
    "Der Beobachtungsraum ist ein 3-dimensionaler Vektor\n",
    "\n",
    "- [0] : Kosinus des Winkels (cos(Œ∏))\n",
    "\n",
    "- [1] : Sinus des Winkels (sin(Œ∏))\n",
    "\n",
    "- [2] : Winkelgeschwindigkeit\n",
    "\n",
    "<img src=\".\\Sinus_und_Kosinus_am_Einheitskreis_pendulum.png\" alt=\"Einheitskreis\" width=\"200\">\n",
    "\n",
    "**Aktionsraum** \n",
    "\n",
    "Eine kontinuierliche Aktion\n",
    "\n",
    "- Eine Kraft (Drehmoment) zwischen -2 und 2 wird auf das Pendel ausge√ºbt.\n",
    "\n",
    "**Belohnung**  \n",
    "\n",
    "- Die Belohnung ist definiert als:\n",
    "`-(Œ∏^2 + 0.1 * Œ∏_dot^2 + 0.001 * action^2)`\n",
    "\n",
    "Dabei ist `Œ∏` der Winkel des Pendels, `Œ∏_dot` die Winkelgeschwindigkeit und `action` das ausge√ºbte Drehmoment.\n",
    "\n",
    "- Die Belohnung ist umso geringer (negativer), je weiter das Pendel von der aufrechten Position entfernt ist und je gr√∂√üer die Geschwindigkeit und das aufgebrachte Drehmoment sind.\n",
    "\n",
    "**Episodenbeendigung** \n",
    "- Eine Episode endet nach 200 Zeitschritten.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåé **Zuf√§llige Altion-Auswahl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate(env, num_episodes=100):\n",
    "    returns = []\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        terminated = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done and not terminated:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        \n",
    "        returns.append(total_reward)\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    \n",
    "    return returns\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "returns = evaluate(env, num_episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuf√§lliges Samplen anzeigen (PyGame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1\n",
    "\n",
    "# Erstelle die Umgebung mit Rendering-Modus \"human\"\n",
    "env = gym.make('Pendulum-v1', render_mode=\"human\")\n",
    "\n",
    "# Liste zum Speichern der Gesamt-Belohnungen jeder Episode\n",
    "returns = []\n",
    "\n",
    "# Durchlaufe die festgelegte Anzahl an Episoden\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state, info = env.reset()  # Umgebung zur√ºcksetzen\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    # Solange die Episode nicht beendet ist, f√ºhre Aktionen aus\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # W√§hle zuf√§llige Aktion\n",
    "        state, reward, done, terminated, info = env.step(action)\n",
    "        done = done or terminated  # Beende die Episode, falls sie abgeschlossen oder terminiert ist\n",
    "        total_reward += reward  # Sammle die Gesamtbelohnung\n",
    "\n",
    "    returns.append(total_reward)  # Speichere die Belohnung der Episode\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "# Schlie√üe die Umgebung nach den Episoden\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø **Daten-Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere Konstanten f√ºr die Spaltennamen\n",
    "COS_ANGLE = \"cosAngle\"\n",
    "SIN_ANGLE = \"sinAngle\"\n",
    "ANG_VEL = \"angVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "def sample_data(episodes=10000):\n",
    "    # Erstelle die Pendulum-Umgebung\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    \n",
    "    # Erstelle eine leere Liste zum Speichern der √úberg√§nge (Transitions)\n",
    "    transitions = []\n",
    "\n",
    "    # Daten sammeln\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            step += 1\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            transitions.append({COS_ANGLE: obs[0], SIN_ANGLE: obs[1], \n",
    "                                ANG_VEL: obs[2], EPISODE: episode, \n",
    "                                STEP: step, ACTION: action[0]})\n",
    "\n",
    "            obs, reward, done, terminated, _ = env.step(action)\n",
    "            done = done or terminated  # Beende, wenn die Episode abgeschlossen oder terminiert ist\n",
    "\n",
    "    # Konvertiere die Liste der √úberg√§nge in ein Pandas DataFrame\n",
    "    return pd.DataFrame(transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df = sample_data(episodes=200)\n",
    "print(\"L√§nge: \", len(df))\n",
    "print(60*\"-\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10,15), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D-Histogramm von cosAngle vs sinAngle\n",
    "plt.figure(figsize=(10, 7))\n",
    "h = plt.hist2d(df[COS_ANGLE], df[SIN_ANGLE], bins=40, norm=LogNorm(), cmap='Reds')\n",
    "plt.colorbar(h[3])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cosinus des Winkels\")\n",
    "plt.ylabel(\"Sinus des Winkels\")\n",
    "plt.title(\"2D-Histogramm von Cosinus vs Sinus des Winkels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è **Daten ausschneiden**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Werte in folgendem Bild dienen nur als Orientierung f√ºr das Winkel zu Bogenma√ü Verh√§ltnis, dies muss gem√§√ü der Definition des Pendulum Environmens aber abgewandelt werden. Wir wollen dann den Winkelbereich [+60¬∞, +120¬∞] herausfiltern, um diesen Bereich f√ºr das Modell unbekannt zu machen.\n",
    "\n",
    "Wir schneiden den folgenden Bereich aus den Daten:\n",
    "( $\\frac{1}{2}$ , $\\frac{\\sqrt{3}}{2}$ ) bis ($-\\frac{1}{2}$ ,$\\frac{\\sqrt{3}}{2}$ ) (vgl. Abb. unten, wobei die Abbildung gedreht werden muss --> custom Abbildung folgt noch). \n",
    "\n",
    "![EInheitskreis](./Einheitskreis_Winkel.png)  \n",
    "\n",
    "Beachte, das Bogenma√ü ist hier anders definiert. Nach dem Pendulum Environment ist 0¬∞ definiert als das Pendel in der upright position.\n",
    "Damit sind alle anderen Werte von *theta* im Intervall [$-\\pi$, $\\pi$]. Positive Werte deuten eine Position im rechten Bereich des Pendulumkreises an, negative wiederum deute auf eine aktuelle Position im linken Bereich des Pendulumkreises an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne den Winkel (theta) basierend auf den x- und y-Werten\n",
    "df['theta_winkel'] = np.arctan2(df[SIN_ANGLE], df[COS_ANGLE])\n",
    "\n",
    "### Datenanalyse (optional)\n",
    "    #print(df.head())\n",
    "    #print(df[COS_ANGLE].max())\n",
    "    #print(df[COS_ANGLE].min())\n",
    "    #print(df[SIN_ANGLE].max())\n",
    "    #print(df[SIN_ANGLE].min())\n",
    "    #print(df['theta_winkel'].max())\n",
    "    #print(df['theta_winkel'].min())\n",
    "    #df['theta_rounded'] = np.round(df['theta_winkel'], decimals=3)\n",
    "    #print(df[df['theta_rounded'] == 0])\n",
    "\n",
    "### Definiere die Winkelgrenzen\n",
    "lower_bound_theta_winkel = np.pi / 3        # +60¬∞\n",
    "upper_bound_theta_winkel = 2 * np.pi / 3   # +120¬∞\n",
    "\n",
    "### Filter f√ºr den Bereich auf dem Einheitskreis\n",
    "cut_out_df = df[(df['theta_winkel'] <= lower_bound_theta_winkel) | (df['theta_winkel'] >= upper_bound_theta_winkel)]\n",
    "print(len(cut_out_df))\n",
    "print(cut_out_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check um richtiges cutten zu gew√§hrleisten nur als Check\n",
    "cut_out_df_check = df[(df['theta_winkel'] > lower_bound_theta_winkel) & (df['theta_winkel'] < upper_bound_theta_winkel)]\n",
    "print(cut_out_df_check.head())\n",
    "print(len(cut_out_df_check))\n",
    "\n",
    "### Weiterf√ºhrende Analyseoptionen\n",
    "    #print(lower_bound_theta_winkel)\n",
    "    #print(upper_bound_theta_winkel)\n",
    "    #Kontrolle der x und y Koordinaten\n",
    "    #print(cut_out_df_check[COS_ANGLE].min())\n",
    "    #print(cut_out_df_check[COS_ANGLE].max())\n",
    "    #print(cut_out_df_check[SIN_ANGLE].min())\n",
    "    #zur Kontrolle m√ºssen die Ergebnisse folgendes aufweisen: \n",
    "    #cosAngle im Intervall [-0.5, 0.5] und sinAngle > sqrt(3)/2 = 0.866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **Datendichte**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimation Model\n",
    "Das Datendichtemodell wird als Kernel Density Estimation umgesetzt.  \n",
    "Im Folgenden wird insbesondere auch der optimale bandwith-Parameter f√ºr die Kernel Density Estimation betrachtet.  \n",
    "Au√üerdem werden verschieden Methoden zur Visualisierung eingesetzt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for kernel density\n",
    "from sklearn.neighbors import KernelDensity\n",
    "#for visualisation (3D plot, but not necessary)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "#for exploring the optimal bandwith parameter\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit folgendem Datendichtemodell *kde* kann f√ºr die generierten Daten die Datendichte berechnet werden. Diese Datendichtemodell kann daraufhin auf alle Daten bzw auch neu gesampelte Daten angewendet werden. Insbesondere ist keine neue Berechnung des Datendichtemodells notwendig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kernel Density Modell (rausgecuttete Daten werden ber√ºcksichtigt)\n",
    "# features: die betrachteten Parameter, f√ºr die eine Datendichte berechnet werden soll\n",
    "features = cut_out_df[[COS_ANGLE, SIN_ANGLE, ANG_VEL]]\n",
    "# Fit KDE model\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth = 0.1)  # optimal bandwidth will be determined below\n",
    "kde.fit(features)\n",
    "\n",
    "# calculate density score\n",
    "log_density = kde.score_samples(features)\n",
    "density = np.exp(log_density)\n",
    "cut_out_df['kde_density'] = density\n",
    "\n",
    "#plots for visualisation\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = cut_out_df[COS_ANGLE]\n",
    "y = cut_out_df[SIN_ANGLE]\n",
    "z = cut_out_df['kde_density']\n",
    "\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis', marker='o')\n",
    "ax.set_xlabel(COS_ANGLE)\n",
    "ax.set_ylabel(SIN_ANGLE)\n",
    "ax.set_zlabel('Density')\n",
    "plt.title('3D KDE-Plot')\n",
    "plt.show()\n",
    "cut_out_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best Bandwith Parameter\n",
    "#### Cross Validation\n",
    "Testen auf verschieden Bandwith Parameter. Bewertung durch den GridSearchCV Algorithmus (cross validation, 5-fold, betrachtet nur den mean)  \n",
    "Alternative Bewertung anhand des Qutienten von Mittelwert (wie gut fittet das Modell) und Standardabweichung (wie stabil ist das Modell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cut_out_df[[COS_ANGLE, SIN_ANGLE, ANG_VEL]]\n",
    "bandwidths = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1]\n",
    "\n",
    "\n",
    "# GridSearchCV zur Bandwidth-Optimierung\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n",
    "                    {'bandwidth': bandwidths},\n",
    "                    cv=5)  # 5-fold is default value\n",
    "#macht es sinn,auf unterschiedliche verteilungen zu testen (also z.b. nicht Gaussian)\n",
    "\n",
    "grid.fit(features)\n",
    "\n",
    "best_bandwidth = grid.best_params_['bandwidth']\n",
    "print(f'Optimale Bandwidth: {best_bandwidth}')\n",
    "\n",
    "# anschauliche Ausgabe der Ergebnisse\n",
    "cv_results = grid.cv_results_\n",
    "\n",
    "mean_scores = cv_results['mean_test_score']\n",
    "std_scores = cv_results['std_test_score']\n",
    "bandwidths = cv_results['param_bandwidth']\n",
    "\n",
    "# Berechne den Quotienten als Alternative zur Bewertung der optimalen Bandwith\n",
    "quotients = mean_scores / std_scores\n",
    "\n",
    "# Erstelle ein DataFrame zur √ºbersichtlichen Darstellung\n",
    "results_df = pd.DataFrame({\n",
    "    'Bandwidth': bandwidths,\n",
    "    'Mean Score': mean_scores,\n",
    "    'Std Score': std_scores,\n",
    "    'Mean / Std Quotient': quotients\n",
    "})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kdeplot (seaborn)\n",
    "Anhand des Plots kann durch Visualisierung ein passender bandwith parameter gefunden werden. Insbesondere soll sowohl overfitting als auch underfitting vermieden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot Density for both parameters\n",
    "sns.kdeplot(cut_out_df[COS_ANGLE], bw_adjust=0.1)\n",
    "plt.title('KDE f√ºr cosAngle')\n",
    "plt.xlabel('cosAngle')\n",
    "plt.ylabel('Dichte')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(cut_out_df[SIN_ANGLE], bw_adjust=0.1)\n",
    "plt.title('KDE f√ºr sinAngle')\n",
    "plt.xlabel('sinAngle')\n",
    "plt.ylabel('Dichte')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(cut_out_df[ANG_VEL], bw_adjust=0.1)\n",
    "plt.title('KDE f√ºr angVel')\n",
    "plt.xlabel('angVel')\n",
    "plt.ylabel('Dichte')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the data density model (kde) to any newly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[[COS_ANGLE, SIN_ANGLE, ANG_VEL]]\n",
    "\n",
    "# Berechne die Dichte f√ºr die gesampelten Daten\n",
    "log_density = kde.score_samples(features)\n",
    "density_sampled_data = np.exp(log_density)\n",
    "df['kde_density'] = density_sampled_data\n",
    "\n",
    "#print(df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = df[COS_ANGLE]\n",
    "y = df[SIN_ANGLE]\n",
    "z = df['kde_density']\n",
    "\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis', marker='o')\n",
    "ax.set_xlabel(COS_ANGLE)\n",
    "ax.set_ylabel(SIN_ANGLE)\n",
    "ax.set_zlabel('Density')\n",
    "plt.title('3D KDE-Plot')\n",
    "plt.show()\n",
    "cut_out_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è **Model Based RL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Training das Zustands√ºbergangsmodells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=4\n",
    "input_col = [COS_ANGLE, SIN_ANGLE, ANG_VEL, ACTION]\n",
    "target_col = [COS_ANGLE, SIN_ANGLE, ANG_VEL]\n",
    "\n",
    "train_data, val_data, input_shape, mean_in, std_in, mean_out, std_out =  \\\n",
    "            utils.prepare_data(df, input_col, target_col, window_size=window_size, training_pattern_percent=0.7)\n",
    "\n",
    "print (\"Input-Shape: \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape, optimizer=tf.keras.optimizers.Adam()):\n",
    "\n",
    "    print(f\"mean in = {mean_in}, std in = {std_in}, mean out = {mean_out}, std out = {std_out}\")\n",
    "\n",
    "    single_step_model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Verwende die benutzerdefinierte Standardisierungsschicht\n",
    "    single_step_model.add(NormalizeLayer(mean=mean_in, std=std_in, input_shape=input_shape))\n",
    "    single_step_model.add(tf.keras.layers.LSTM(50, input_shape=input_shape, dtype=np.float32))\n",
    "    single_step_model.add(tf.keras.layers.Dense(len(mean_out), activation=\"linear\"))\n",
    "\n",
    "    single_step_model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return single_step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"model.keras\"\n",
    "max_epochs = 2500\n",
    "steps_per_epoch = 100\n",
    "validation_steps = 100\n",
    "validation_freq = 1\n",
    "\n",
    "# Callbacks\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=250, restore_best_weights=True, verbose=True)\n",
    "mc_trainLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestTrainLoss.keras\" % modelpath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "mc_valLoss_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"%s_bestValLoss.keras\" % modelpath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./model_logs_tb\", histogram_freq=1)\n",
    "\n",
    "# Modell erstellen\n",
    "step_model = build_single_step_model(mean_in, std_in, mean_out, std_out, input_shape)\n",
    "\n",
    "# Modell trainieren\n",
    "history = step_model.fit(train_data, epochs=max_epochs, steps_per_epoch=steps_per_epoch,\n",
    "                        validation_data=val_data, validation_steps=validation_steps, validation_freq=validation_freq,\n",
    "                        callbacks=[mc_trainLoss_callback, mc_valLoss_callback, es_callback, tensorboard_callback])\n",
    "\n",
    "# Modell speichern\n",
    "step_model.save(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Modellevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEval = sample_data(episodes=1)\n",
    "dfEval = dfEval[dfEval.episode==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=4\n",
    "output_min = [-1, -1, -8]\n",
    "output_max = [1, 1, 8]\n",
    "\n",
    "# Modell laden\n",
    "model = tf.keras.models.load_model(\"model.keras_bestValLoss.keras\", compile=False)\n",
    "\n",
    "# FIFO-Puffer, der den Zustand des neuronalen Netzes speichert\n",
    "stateBuffer = collections.deque(maxlen=window_size)\n",
    "\n",
    "# Ausgaben des neuronalen Netzwerks werden hier gespeichert\n",
    "transitions = []\n",
    "\n",
    "for i in range(len(dfEval)): \n",
    "                            \n",
    "    # Sch√§tzung des ersten Zustands\n",
    "    if i < window_size: \n",
    "        state_data = np.float32([dfEval[COS_ANGLE].values[i], dfEval[SIN_ANGLE].values[i],\n",
    "                               dfEval[ANG_VEL].values[i],\n",
    "                               dfEval[ACTION].values[i]])\n",
    "        stateBuffer.append(state_data)\n",
    "    \n",
    "    # Vorhersage des nachfolgenden Zustands\n",
    "    else: \n",
    "        \n",
    "        state = np.array([list(stateBuffer)])\n",
    "        if i==5:\n",
    "            print(state)\n",
    "        netOutput = model.predict(np.float32(state))[0]\n",
    "\n",
    "        # √úberpr√ºfen, ob eine Begrenzung √ºberschritten wurde\n",
    "        if np.any(netOutput < output_min) or np.any(netOutput > output_max):\n",
    "            print (\"Begrenzung √ºberschrritten bei Schritt: \", i)\n",
    "            print(netOutput)\n",
    "        \n",
    "        # Begrenzung der Ausgabe auf begrenzenden Datenbereich\n",
    "        netOutput = np.clip(netOutput, output_min, output_max)\n",
    "        \n",
    "        ##########################\n",
    "        # Sollten wir bei der Begrenzung wegen Sinus und Kosinus nicht kleiner/gr√∂√üer ohne Gleich verwenden?\n",
    "        #######################\n",
    "\n",
    "        # Daten zum Plotten hinzuf√ºgen\n",
    "        transitions.append({\n",
    "            COS_ANGLE: netOutput[0], SIN_ANGLE: netOutput[1],\n",
    "            ANG_VEL: netOutput[2]\n",
    "        })\n",
    "        \n",
    "        # RNN-Zustand aktualisieren\n",
    "        stateBuffer.append(np.float32([netOutput[0], netOutput[1], \n",
    "                                       netOutput[2], \n",
    "                                       dfEval[ACTION].values[i]]))\n",
    "        \n",
    "dfNet = pd.DataFrame(transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots (4, 1, figsize=(10,8))\n",
    "\n",
    "fields = [COS_ANGLE, SIN_ANGLE, ANG_VEL]\n",
    "\n",
    "for i in range (len(fields)):\n",
    "    f = fields[i]\n",
    "    axs[i].plot(range (len(dfNet)), dfEval[f].values[window_size:window_size+len(dfNet)], label=f)\n",
    "    axs[i].plot(range (len(dfNet)), dfNet[f].values, label=\"prediction\", ls=\"--\")\n",
    "    axs[i].grid()\n",
    "    axs[i].legend(loc=\"best\")\n",
    "    \n",
    "axs[3].plot(range (len(dfNet)), dfEval[ACTION].values[window_size:window_size+len(dfNet)], label=ACTION)\n",
    "axs[3].grid()\n",
    "axs[3].legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë£ **Policies trainieren**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep Deterministic Policy Gradient (DDPG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs_modelBased_DDPG\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Pendulum_Custom_Environment(model_filename=\"model.keras\", window_size=4)\n",
    "env = Monitor(env, log_dir) # required for using callback functions during training\n",
    "env = DummyVecEnv([lambda: env]) # optionally state normalization\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True) # State normalization\n",
    "env.render_mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.ddpg import MlpPolicy\n",
    "\n",
    "# Define the model\n",
    "model_ddpg = DDPG(MlpPolicy, env, verbose=1, device=\"auto\")\n",
    "\n",
    "# Define the callback\n",
    "callback_ddpg = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=log_dir, avg_episodes=30, vec_norm_env=env)\n",
    "\n",
    "# Train the model\n",
    "model_ddpg.learn(total_timesteps=100000, callback=callback_ddpg)\n",
    "\n",
    "# Save the model\n",
    "model_ddpg.save(os.path.join(log_dir, 'ddpg_model'))\n",
    "\n",
    "# Save normalization parameters\n",
    "env.save(os.path.join(log_dir, 'ddpg_env_normalizations'))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Soft Actor Critic (SAC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs_modelBased_SAC\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Pendulum_Custom_Environment(model_filename=\"model.keras\", window_size=4)\n",
    "env = Monitor(env, log_dir) # required for using callback functions during training\n",
    "env = DummyVecEnv([lambda: env])# optionally state normalization\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True) # State normalization\n",
    "env.render_mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac import MlpPolicy\n",
    "\n",
    "# Define the model\n",
    "model_sac = SAC(MlpPolicy, env, verbose=1, device=\"auto\")\n",
    "\n",
    "# Define the callback\n",
    "callback_sac = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=log_dir, avg_episodes=30, vec_norm_env=env)\n",
    "\n",
    "# Train the model\n",
    "model_sac.learn(total_timesteps=100000, callback=callback_sac)\n",
    "\n",
    "# Save the model\n",
    "model_sac.save(os.path.join(log_dir, 'sac_model'))\n",
    "\n",
    "# Save normalization parameters\n",
    "env.save(os.path.join(log_dir, 'sac_env_normalizations'))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Asynchronous Advantage Actor Critic (A3C)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs_modelBased_A2C\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Pendulum_Custom_Environment(model_filename=\"model.keras\", window_size=4)\n",
    "env = Monitor(env, log_dir) # required for using callback functions during training\n",
    "env = DummyVecEnv([lambda: env])# optionally state normalization\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True) # State normalization\n",
    "env.render_mode = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.a2c import MlpPolicy\n",
    "\n",
    "# Define the model\n",
    "model_a2c = A2C(MlpPolicy, env, verbose=1, device=\"auto\")\n",
    "\n",
    "# Define the callback\n",
    "callback_a2c = SaveOnBestTrainingRewardCallback(check_freq=5000, log_dir=log_dir, avg_episodes=30, vec_norm_env=env)\n",
    "\n",
    "# Train the model\n",
    "model_a2c.learn(total_timesteps=100000, callback=callback_a2c)\n",
    "\n",
    "# Save the model\n",
    "model_a2c.save(os.path.join(log_dir, 'a2c_model'))\n",
    "\n",
    "# Save normalization parameters\n",
    "env.save(os.path.join(log_dir, 'a2c_env_normalizations'))\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë©‚Äçüî¨ **Policies evaluieren** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# um sicherzustellen, welche trainierte Policy wir laden, da log_dir f√ºr jedes Training neu gesetzt wird\n",
    "log_dir = \"logs_modelBased_DDPG\"\n",
    "#log_dir = \"logs_modelBased_SAC\"\n",
    "#log_dir = \"logs_modelBased_A2C\"\n",
    "print (log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ergebnisse plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 100000\n",
    "results_plotter.plot_results([log_dir], time_steps, results_plotter.X_TIMESTEPS, \"%s modelBased Learning Pendulum\" % log_dir.split('_')[-1])\n",
    "plt.savefig(\"%s/graph.png\" % log_dir)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anwenden der Policies auf das originale Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Change (...).load \n",
    "# DDPG, SAC, A2C\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "best_policy_modelBased = DDPG.load(\"%s/best_model.zip\" % log_dir)\n",
    "#best_policy_modelBased = SAC.load(\"%s/best_model.zip\" % log_dir)\n",
    "#best_policy_modelBased = A2C.load(\"%s/best_model.zip\" % log_dir)\n",
    "\n",
    "## W√§hlen, ob wir mit rendering testen wollen\n",
    "## Falls wir ohne rendern testen:\n",
    "#env = DummyVecEnv([lambda: gymmake(\"Pendulum-v1\")])\n",
    "## Falls wir mit rendern testen \n",
    "\n",
    "# None oder human\n",
    "render_mode = \"human\"\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(\"Pendulum-v1\", render_mode = render_mode)])\n",
    "\n",
    "# Load the saved statistics\n",
    "env = VecNormalize.load(\"%s/best_model.env_normalizations\" % log_dir, env)\n",
    "\n",
    "\n",
    "env.training=False\n",
    "\n",
    "## hier k√∂nnen wir setzen, welche Werte wir normalisieren wollen (je nach Bedarf auskommentieren)\n",
    "## wenn wir die Normalisierung ausstellen, k√∂nnen wir die Werte mit der reward funktion replizieren!! \n",
    "## --> dann funktioniert aber das Model nicht, da wir f√ºr die Prediction die normalisierten states brauchen\n",
    "\n",
    "#hier k√∂nnen wir mit False die Werte aus dem Trainingsgraphen vergleichen\n",
    "env.norm_reward = False\n",
    "\n",
    "#diese Option w√ºrde die States nicht normalisieren, damit kann der reward zwar formeltechnisch berechnet werden, aber wir haben keine richtige policy mehr\n",
    "#env.norm_obs = False\n",
    "\n",
    "##Falls wir mehrfach ausf√ºhren wollen, dann mit for i in range\n",
    "#for i in range (5): \n",
    "state = env.reset()\n",
    "done=False\n",
    "cumReward = 0\n",
    "steps=0\n",
    "\n",
    "while not done: \n",
    "    steps += 1\n",
    "    action, _states = best_policy_modelBased.predict(state)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done: \n",
    "        print (f\"Episode terminated after {steps} steps\")\n",
    "    #done k√∂nnen wir rausschmei√üen glaub ich\n",
    "    cumReward += reward\n",
    "\n",
    "    env.render() ## <- comment out in Google colab\n",
    "\n",
    "print (\"steps=%d, cumReward=%.3f\" % (steps, cumReward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **Autoren**: Maximilian Schieder, Leon Lantz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
