{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/LMU_Muenchen_Logo.svg\" alt=\"Image\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "*Gruppe A - Thema 5*\n",
    "\n",
    "### **RL-Policy-Training unter Vermeidung von Zust√§nden mit sehr geringer Datendichte**\n",
    "\n",
    "- Erzeugung eines Datensatzes f√ºr das Absch√§tzung der Datendichte\n",
    "\n",
    "- Sampeln eines Gym-Environments \n",
    "    - Entfernen vordefinierter Bereiche, die z.B. durch die eine optimale Policy laufen w√ºrde\n",
    "\n",
    "- Modell-based RL\n",
    "\n",
    "- Datendichte im Reward abbilden, aber nicht √ºbergewichten\n",
    "\n",
    "**Dozent:** [Dr. Michel Tokic](https://www.tokic.com/)\n",
    "\n",
    "**Studenten:** Maximilian Schieder, Leon Lantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Konzept**\n",
    "\n",
    "Offline Reinforcement Learning!\n",
    "\n",
    "1. Sampling im Environment\n",
    "    - Ramdom oder gezielt (Heuristik, Rewards von Environment √§ndern)\n",
    "    - Zustand und entsprechende Aktion jedes Schrittes speichern\n",
    "2. Bestimmung der Datendichte\n",
    "3. Entfernung von Zust√§nden mit geringer Datendichte\n",
    "4. Verwendung des gefilterten Datensatzes f√ºr das Training\n",
    "5. Model-Based RFL ...\n",
    "\n",
    "Anstatt seltene Zust√§nde komplett zu entfernen, k√∂nnte man sie auch weniger stark gewichten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ **Imports**\n",
    "\n",
    "- `tensorflow` as `tf` for building and training machine learning models\n",
    "\n",
    "- `numpy` as `np` for numerical operations and handling arrays\n",
    "\n",
    "- `random` for generating random numbers and randomizing data\n",
    "\n",
    "- `gym` for creating and interacting with reinforcement learning environments (OpenAI)\n",
    "\n",
    "- `pandas` as `pd` for data manipulation and analysis\n",
    "\n",
    "- `matplotlib.colors` for color normalization in plots\n",
    "\n",
    "- `matplotlib.pyplot` as `plt` for creating visualizations and plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the code is run, the random numbers generated will be the same, leading to **reproducible results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed (0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/mountain_car.gif\" alt=\"Mountain Car GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Mountain Car MDP [**(Documentation)**](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "- **Goal**: Reach the goal state at the top of the right hill\n",
    "- **Problem**: The car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum\n",
    "- **Starting Position**: Car starts stochastically at the bottom of a valley\n",
    "\n",
    "### **Observation Space**\n",
    "- 0: position of the car along the x-axis [-1.2, 0.6]\n",
    "- 1: velocity of the car [-0.07, 0.07]\n",
    "\n",
    "### **Action Space**\n",
    "\n",
    "Discrete deterministic actions\n",
    "- 0: Accelerate to the left\n",
    "- 1: Don‚Äôt accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "### **Reward**\n",
    "\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
    "\n",
    "### **Episode End**\n",
    "- **Termination**: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "- **Truncation**: The length of the episode is 200.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "CAR_POS = \"carPos\"\n",
    "CAR_VEL = \"carVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "\n",
    "#OPTIONS: Set by User !!!\n",
    "RENDERED = True\n",
    "HEURISTIC = False\n",
    "\n",
    "\n",
    "def heuristic_policy(velocity):\n",
    "    if velocity >= 0.0:\n",
    "        return 2 # Accelerate right\n",
    "    else:\n",
    "        return 0 # Accelerate left\n",
    "\n",
    "\n",
    "def sample_data(episodes=1, seed=0):\n",
    "\n",
    "    if(RENDERED): env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "    else: env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    env.reset()\n",
    "\n",
    "    ### Create empty Pandas dataset\n",
    "    transitions = []\n",
    "\n",
    "    ### SAMPLE DATA\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while step < 200 and not done:\n",
    "            step += 1\n",
    "\n",
    "            if(step == 1):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                if (HEURISTIC): action = heuristic_policy(obs[1])\n",
    "                else: action = env.action_space.sample()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            transitions.append(\n",
    "                {\n",
    "                    CAR_POS: obs[0],\n",
    "                    CAR_VEL: obs[1],\n",
    "                    EPISODE: episode,\n",
    "                    STEP: step,\n",
    "                    ACTION: action,\n",
    "                }\n",
    "            )\n",
    "        print(\"Steps: \", step)\n",
    "\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "df = sample_data(episodes=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderer Ansatz:**\n",
    "\n",
    "-> Reward √§ndern. z.B. wenn Car halbwegs gute Ergebnisse erzielt (weit oben auf H√ºgel) Reward +1 setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: red;\">Zust√§nde nahe des Gipfels sind <b>selten, aber wichtig</b> um das Auto in den Gipfelbereich zu bringen. Das Vermeiden solcher Zust√§nde k√∂nnte dazu f√ºhren, dass das Modell nicht lernt, wie man erfolgreich den Gipfel erreicht.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoren**: Maximilian Schieder, Leon Lantz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
