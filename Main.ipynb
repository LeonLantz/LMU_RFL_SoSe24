{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/LMU_Muenchen_Logo.svg\" alt=\"Image\" width=\"200\" style=\"float: right;\">\n",
    "\n",
    "*Gruppe A - Thema 5*\n",
    "\n",
    "### **RL-Policy-Training unter Vermeidung von Zust√§nden mit sehr geringer Datendichte**\n",
    "\n",
    "- Erzeugung eines Datensatzes f√ºr das Absch√§tzung der Datendichte\n",
    "\n",
    "- Sampeln eines Gym-Environments \n",
    "    - Entfernen vordefinierter Bereiche, die z.B. durch die eine optimale Policy laufen w√ºrde\n",
    "\n",
    "- Modell-based RL\n",
    "\n",
    "- Datendichte im Reward abbilden, aber nicht √ºbergewichten\n",
    "\n",
    "**Dozent:** [Dr. Michel Tokic](https://www.tokic.com/)\n",
    "\n",
    "**Studenten:** Maximilian Schieder, Leon Lantz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Konzept**\n",
    "\n",
    "Offline Reinforcement Learning!\n",
    "\n",
    "1. Sampling im Environment\n",
    "    - Ramdom oder gezielt (Heuristik, Rewards von Environment √§ndern)\n",
    "    - Zustand und entsprechende Aktion jedes Schrittes speichern\n",
    "2. Bestimmung der Datendichte\n",
    "3. Entfernung von Zust√§nden mit geringer Datendichte\n",
    "4. Verwendung des gefilterten Datensatzes f√ºr das Training\n",
    "5. Model-Based RFL ...\n",
    "\n",
    "Anstatt seltene Zust√§nde komplett zu entfernen, k√∂nnte man sie auch weniger stark gewichten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ **Imports**\n",
    "\n",
    "- `tensorflow` as `tf` for building and training machine learning models\n",
    "\n",
    "- `numpy` as `np` for numerical operations and handling arrays\n",
    "\n",
    "- `random` for generating random numbers and randomizing data\n",
    "\n",
    "- `gym` for creating and interacting with reinforcement learning environments (OpenAI)\n",
    "\n",
    "- `pandas` as `pd` for data manipulation and analysis\n",
    "\n",
    "- `matplotlib.colors` for color normalization in plots\n",
    "\n",
    "- `matplotlib.pyplot` as `plt` for creating visualizations and plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the code is run, the random numbers generated will be the same, leading to **reproducible results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed (0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöó **Mountain Car**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/mountain_car.gif\" alt=\"Mountain Car GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Mountain Car MDP [**(Documentation)**](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "- **Goal**: Reach the goal state at the top of the right hill\n",
    "- **Problem**: The car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum\n",
    "- **Starting Position**: Car starts stochastically at the bottom of a valley\n",
    "\n",
    "#### **Observation Space**\n",
    "- 0: position of the car along the x-axis [-1.2, 0.6]\n",
    "- 1: velocity of the car [-0.07, 0.07]\n",
    "\n",
    "#### **Action Space**\n",
    "\n",
    "Discrete deterministic actions\n",
    "- 0: Accelerate to the left\n",
    "- 1: Don‚Äôt accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "#### **Reward**\n",
    "\n",
    "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
    "\n",
    "#### **Episode End**\n",
    "- **Termination**: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "- **Truncation**: The length of the episode is 200.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py\n",
    "CAR_POS = \"carPos\"\n",
    "CAR_VEL = \"carVel\"\n",
    "EPISODE = \"episode\"\n",
    "STEP = \"step\"\n",
    "ACTION = \"action\"\n",
    "\n",
    "\n",
    "#OPTIONS: Set by User !!!\n",
    "RENDERED = True\n",
    "HEURISTIC = False\n",
    "\n",
    "\n",
    "def heuristic_policy(velocity):\n",
    "    if velocity >= 0.0:\n",
    "        return 2 # Accelerate right\n",
    "    else:\n",
    "        return 0 # Accelerate left\n",
    "\n",
    "\n",
    "def sample_data(episodes=1, seed=0):\n",
    "\n",
    "    if(RENDERED): env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "    else: env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    env.reset()\n",
    "\n",
    "    ### Create empty Pandas dataset\n",
    "    transitions = []\n",
    "\n",
    "    ### SAMPLE DATA\n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "\n",
    "        while step < 200 and not done:\n",
    "            step += 1\n",
    "\n",
    "            if(step == 1):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                if (HEURISTIC): action = heuristic_policy(obs[1])\n",
    "                else: action = env.action_space.sample()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            transitions.append(\n",
    "                {\n",
    "                    CAR_POS: obs[0],\n",
    "                    CAR_VEL: obs[1],\n",
    "                    EPISODE: episode,\n",
    "                    STEP: step,\n",
    "                    ACTION: action,\n",
    "                }\n",
    "            )\n",
    "        print(\"Steps: \", step)\n",
    "\n",
    "    return pd.DataFrame(transitions)\n",
    "\n",
    "df = sample_data(episodes=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anderer Ansatz:**\n",
    "\n",
    "-> Reward √§ndern. z.B. wenn Car halbwegs gute Ergebnisse erzielt (weit oben auf H√ºgel) Reward +1 setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color: red;\">Zust√§nde nahe des Gipfels sind <b>selten, aber wichtig</b> um das Auto in den Gipfelbereich zu bringen. Das Vermeiden solcher Zust√§nde k√∂nnte dazu f√ºhren, dass das Modell nicht lernt, wie man erfolgreich den Gipfel erreicht.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåë **Lunar Lander**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gymlibrary.dev/_images/lunar_lander.gif\" alt=\"Lunar Lander GIF\" width=\"400\" >\n",
    "\n",
    "Gym Environment -> Lunar Lander [**(Documentation)**](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n",
    "\n",
    "- **Ziel**: Das Ziel besteht darin, den Lander sicher auf dem Landepad zu landen.\n",
    "\n",
    "- **Ausgangsposition**: Der Lander startet in der oberen Mitte des Ansichtsbereichs mit einer zuf√§lligen Anfangskraft, die auf seinen Schwerpunkt angewendet wird.\n",
    "\n",
    "#### **Beobachtungsraum**\n",
    "Der Beobachtungsraum ist ein 8-dimensionaler Vektor\n",
    "- [0] : x-Koordinate des Landers\n",
    "- [1] : y-Koordinate des Landers\n",
    "- [2] : lineare Geschwindigkeit in x-Richtung\n",
    "- [3] : lineare Geschwindigkeit in y-Richtung\n",
    "- [4] : Winkel\n",
    "- [5] : Winkelgeschwindigkeit\n",
    "- [6] : linkes Bein in Kontakt mit dem Boden (Boolean)\n",
    "- [7] : rechtes Bein in Kontakt mit dem Boden (Boolean)\n",
    "\n",
    "#### **Aktionsraum**\n",
    "Vier diskrete Aktionen\n",
    "- [0] : nichts tun\n",
    "- [1] : linke Orientierungsturbine feuern\n",
    "- [2] : Hauptturbine feuern\n",
    "- [3] : rechte Orientierungsturbine feuern\n",
    "\n",
    "#### **Belohnung**\n",
    "- Die Belohnung erh√∂ht/verringert sich, je n√§her/weiter der Lander dem Landepad ist.\n",
    "- Die Belohnung erh√∂ht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Die Belohnung verringert sich, je mehr der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Die Belohnung erh√∂ht sich um 10 Punkte f√ºr jedes Bein, das den Boden ber√ºhrt.\n",
    "- Die Belohnung verringert sich um 0,03 Punkte f√ºr jeden Frame, in dem eine Seitenturbine feuert.\n",
    "- Die Belohnung verringert sich um 0,3 Punkte f√ºr jeden Frame, in dem die Hauptturbine feuert.\n",
    "\n",
    "#### **Episodenbeendigung**\n",
    "- Der Lander st√ºrzt ab (der Landerk√∂rper kommt mit dem Mond in Kontakt).\n",
    "- Der Lander ger√§t au√üerhalb des Ansichtsbereichs (x-Koordinate ist gr√∂√üer als 1).\n",
    "- Der Lander ist nicht wach. Laut den Box2D-Dokumenten ist ein K√∂rper, der nicht wach ist, ein K√∂rper, der sich nicht bewegt und mit keinem anderen K√∂rper kollidiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze den Environment Name\n",
    "environment_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üåé **Zuf√§llige Action-Auswahl**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate(env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, terminated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "env = gym.make(environment_name)\n",
    "returns = evaluate(env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/random/episode_{episode}.gif\"\n",
    "    state, info = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Zuf√§llige Auswahl einer Aktion\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, terminated, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:lightgreen;\">Ergebnis:</p>  Bei der Durchf√ºhrung von zehn Episoden in der <b>Lunar Lander-Umgebung</b>, in denen der Agent zuf√§llig Aktionen ausw√§hlt, waren die Ergebnisse erwartungsgem√§√ü schlecht. Der kumulierte Reward ist in der Regel negativ, da zuf√§llige Aktionen selten zu einer erfolgreichen Landung f√ºhren. Diese Ergebnisse zeigen deutlich, wie wichtig durchdachte Strategien und gut trainierte Modelle f√ºr den Erfolg in komplexen Umgebungen wie Lunar Lander sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üèãüèº **Trainiere ein Modell mit dem A2C-Algorithmus** <b style=\"color:red;\">(Optional)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = A2C('MlpPolicy', env, ent_coef=0.1, verbose=1)\n",
    "model.learn(total_timesteps=80000)\n",
    "model.save(\"models/A2C_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üë®‚Äçüè´ **Bewertung der Modelle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "#del model #optional\n",
    "#model = A2C.load('models/A2C_model', env=env)\n",
    "model = A2C.load('models/a2c-LunarLander-v2')\n",
    "# Andere Modelle zur Datengenerierung "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchschnitt √ºber 100 Episoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Anzahl der Episoden f√ºr die Bewertung\n",
    "num_episodes = 100\n",
    "\n",
    "# Funktion zur Bewertung des Modells\n",
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    returns = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "        #print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    \n",
    "    # Durchschnittliche R√ºckgabe berechnen\n",
    "    average_return = sum(returns) / num_episodes\n",
    "    print(f\"Durchschnittlicher Reward √ºber {num_episodes} Episoden: {average_return}\")\n",
    "    return returns\n",
    "\n",
    "# Modell bewerten\n",
    "returns = evaluate_model(model, env, num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichere 10 Episoden als GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "# Funktion zur Aufnahme einer Episode und Speicherung als GIF\n",
    "for episode in range(episodes):\n",
    "    output_path = f\"results/pretrained_model/episode_{episode}.gif\"\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # F√ºge das aktuelle Bild der Frames-Liste hinzu\n",
    "        frames.append(env.render())\n",
    "        \n",
    "        # Vorhersage der Aktion durch das Modell\n",
    "        action, _states = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "    print('Episode:{} Kummulierter Reward:{}'.format(episode, cum_reward))\n",
    "    \n",
    "    with imageio.get_writer(output_path, mode='I', duration=0.03) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "# Schlie√üe das Environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoren**: Maximilian Schieder, Leon Lantz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
